{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pretrain_gpt2.py', '--num-layers', '12', '--hidden-size', '768', '--num-attention-heads', '12', '--seq-length', '2048', '--max-position-embeddings', '2048', '--micro-batch-size', '4', '--global-batch-size', '128', '--lr', '1.5e-4', '--min-lr', '1.5e-5', '--hidden-dropout', '0.0', '--attention-dropout', '0.0', '--train-iters', '100', '--lr-decay-iters', '100', '--lr-decay-style', 'cosine', '--use-parallel-residual', '--weight-decay', '1e-1', '--lr-warmup-fraction', '.01', '--clip-grad', '1.0', '--normalization', 'LayerNorm', '--position-embedding-type', 'rope', '--rotary-percent', '0.25', '--use-mcore-models', '--untie-embeddings-and-output-weights', '--tokenizer-type', 'HFTokenizer', '--vocab-file', '/workspace/gpt-neox-tokenizer.json', '--load', '/data/pythia/pythia-160m-megatron', '--save', '/data/pythia/pythia-160m-megatron/ft-test', '--save-interval', '100', '--finetune']\n",
      "using world size: 1, data-parallel size: 1, context-parallel size: 1 tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  add_bias_linear ................................. True\n",
      "  add_position_embedding .......................... True\n",
      "  add_qkv_bias .................................... False\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_layernorm_1p .............................. False\n",
      "  apply_query_key_layer_scaling ................... False\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  apply_rope_fusion ............................... True\n",
      "  async_tensor_model_parallel_allreduce ........... True\n",
      "  attention_dropout ............................... 0.0\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  auto_detect_ckpt_format ......................... False\n",
      "  barrier_with_L1_time ............................ True\n",
      "  bert_binary_head ................................ True\n",
      "  bert_embedder_type .............................. megatron\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  bias_swiglu_fusion .............................. True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  check_for_nan_in_loss_and_grad .................. True\n",
      "  classes_fraction ................................ 1.0\n",
      "  clip_grad ....................................... 1.0\n",
      "  clone_scatter_output_in_embedding ............... True\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  context_parallel_size ........................... 1\n",
      "  data_cache_path ................................. None\n",
      "  data_parallel_random_init ....................... False\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  data_per_class_fraction ......................... 1.0\n",
      "  data_sharding ................................... True\n",
      "  dataloader_type ................................. single\n",
      "  decoder_num_layers .............................. None\n",
      "  decoder_seq_length .............................. None\n",
      "  delay_grad_reduce ............................... True\n",
      "  delay_param_gather .............................. False\n",
      "  dino_bottleneck_size ............................ 256\n",
      "  dino_freeze_last_layer .......................... 1\n",
      "  dino_head_hidden_size ........................... 2048\n",
      "  dino_local_crops_number ......................... 10\n",
      "  dino_local_img_size ............................. 96\n",
      "  dino_norm_last_layer ............................ False\n",
      "  dino_teacher_temp ............................... 0.07\n",
      "  dino_warmup_teacher_temp ........................ 0.04\n",
      "  dino_warmup_teacher_temp_epochs ................. 30\n",
      "  dist_ckpt_format ................................ torch_dist\n",
      "  distribute_saved_activations .................... False\n",
      "  distributed_backend ............................. nccl\n",
      "  distributed_timeout_minutes ..................... 10\n",
      "  dsparse_anneal .................................. False\n",
      "  dsparse_factor .................................. None\n",
      "  dsparse_finetune ................................ False\n",
      "  dsparse_nblocks ................................. None\n",
      "  dsparse_normalize_mask .......................... False\n",
      "  dsparse_start_t ................................. None\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  enable_one_logger ............................... False\n",
      "  encoder_num_layers .............................. 12\n",
      "  encoder_seq_length .............................. 2048\n",
      "  end_weight_decay ................................ 0.1\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  exit_on_missing_checkpoint ...................... False\n",
      "  exit_signal_handler ............................. False\n",
      "  expert_model_parallel_size ...................... 1\n",
      "  ffn_hidden_size ................................. 3072\n",
      "  finetune ........................................ True\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  fp8 ............................................. None\n",
      "  fp8_amax_compute_algo ........................... most_recent\n",
      "  fp8_amax_history_len ............................ 1\n",
      "  fp8_interval .................................... 1\n",
      "  fp8_margin ...................................... 0\n",
      "  fp8_wgrad ....................................... True\n",
      "  global_batch_size ............................... 128\n",
      "  gradient_accumulation_fusion .................... True\n",
      "  group_query_attention ........................... False\n",
      "  head_lr_mult .................................... 1.0\n",
      "  hidden_dropout .................................. 0.0\n",
      "  hidden_size ..................................... 768\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_h ........................................... 224\n",
      "  img_w ........................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  inference_batch_times_seqlen_threshold .......... 512\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  iter_per_epoch .................................. 1250\n",
      "  kv_channels ..................................... 64\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ /data/pythia/pythia-160m-megatron\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 100\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_progress .................................... False\n",
      "  log_throughput .................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  log_world_size_to_tensorboard ................... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 100\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_init .................................. 0.0\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  manual_gc ....................................... False\n",
      "  manual_gc_eval .................................. True\n",
      "  manual_gc_interval .............................. 0\n",
      "  mask_factor ..................................... 1.0\n",
      "  mask_prob ....................................... 0.15\n",
      "  mask_type ....................................... random\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 2048\n",
      "  max_tokens_to_oom ............................... 12000\n",
      "  merge_file ...................................... None\n",
      "  micro_batch_size ................................ 4\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1.5e-05\n",
      "  mmap_bin_files .................................. True\n",
      "  mock_data ....................................... False\n",
      "  moe_aux_loss_coeff .............................. 0.0\n",
      "  moe_grouped_gemm ................................ False\n",
      "  moe_input_jitter_eps ............................ None\n",
      "  moe_router_load_balancing_type .................. aux_loss\n",
      "  moe_router_topk ................................. 2\n",
      "  moe_token_dropping .............................. False\n",
      "  moe_z_loss_coeff ................................ None\n",
      "  nccl_communicator_config_path ................... None\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_persist_layer_norm ........................... False\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  norm_epsilon .................................... 1e-05\n",
      "  normalization ................................... LayerNorm\n",
      "  num_attention_heads ............................. 12\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_experts ..................................... None\n",
      "  num_layers ...................................... 12\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_query_groups ................................ 1\n",
      "  num_workers ..................................... 2\n",
      "  one_logger_entity ............................... hwinf_dcm\n",
      "  one_logger_project .............................. e2e-tracking\n",
      "  one_logger_run_name ............................. None\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  output_bert_embeddings .......................... False\n",
      "  overlap_grad_reduce ............................. False\n",
      "  overlap_p2p_comm ................................ False\n",
      "  overlap_param_gather ............................ False\n",
      "  override_opt_param_scheduler .................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  perform_initialization .......................... True\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  position_embedding_type ......................... rope\n",
      "  profile ......................................... False\n",
      "  profile_ranks ................................... [0]\n",
      "  profile_step_end ................................ 12\n",
      "  profile_step_start .............................. 10\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  recompute_granularity ........................... None\n",
      "  recompute_method ................................ None\n",
      "  recompute_num_layers ............................ None\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  retro_add_retriever ............................. False\n",
      "  retro_attention_gate ............................ 1\n",
      "  retro_cyclic_train_iters ........................ None\n",
      "  retro_encoder_attention_dropout ................. 0.1\n",
      "  retro_encoder_hidden_dropout .................... 0.1\n",
      "  retro_encoder_layers ............................ 2\n",
      "  retro_num_neighbors ............................. 2\n",
      "  retro_num_retrieved_chunks ...................... 2\n",
      "  retro_return_doc_ids ............................ False\n",
      "  retro_verify_neighbor_count ..................... True\n",
      "  retro_workdir ................................... None\n",
      "  rotary_interleaved .............................. False\n",
      "  rotary_percent .................................. 0.25\n",
      "  rotary_seq_len_interpolation_factor ............. None\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ /data/pythia/pythia-160m-megatron/ft-test\n",
      "  save_interval ................................... 100\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 2048\n",
      "  sequence_parallel ............................... False\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  skip_train ...................................... False\n",
      "  spec ............................................ None\n",
      "  split ........................................... 969, 30, 1\n",
      "  squared_relu .................................... False\n",
      "  standalone_embedding_stage ...................... False\n",
      "  start_weight_decay .............................. 0.1\n",
      "  swiglu .......................................... False\n",
      "  swin_backbone_type .............................. tiny\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  test_data_path .................................. None\n",
      "  timing_log_level ................................ 0\n",
      "  timing_log_option ............................... minmax\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_model ................................. None\n",
      "  tokenizer_type .................................. HFTokenizer\n",
      "  tp_comm_bulk_dgrad .............................. True\n",
      "  tp_comm_bulk_wgrad .............................. True\n",
      "  tp_comm_overlap ................................. False\n",
      "  tp_comm_overlap_cfg ............................. None\n",
      "  tp_comm_split_ag ................................ True\n",
      "  tp_comm_split_rs ................................ True\n",
      "  train_data_path ................................. None\n",
      "  train_iters ..................................... 100\n",
      "  train_samples ................................... None\n",
      "  transformer_impl ................................ local\n",
      "  transformer_pipeline_model_parallel_size ........ 1\n",
      "  untie_embeddings_and_output_weights ............. True\n",
      "  use_checkpoint_args ............................. False\n",
      "  use_checkpoint_opt_param_scheduler .............. False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_dist_ckpt ................................... False\n",
      "  use_distributed_optimizer ....................... False\n",
      "  use_flash_attn .................................. False\n",
      "  use_mcore_models ................................ True\n",
      "  use_one_sent_docs ............................... False\n",
      "  use_parallel_residual ........................... True\n",
      "  use_ring_exchange_p2p ........................... False\n",
      "  use_rotary_position_embeddings .................. False\n",
      "  valid_data_path ................................. None\n",
      "  variable_seq_lengths ............................ False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vision_backbone_type ............................ vit\n",
      "  vision_pretraining .............................. False\n",
      "  vision_pretraining_type ......................... classify\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... /workspace/gpt-neox-tokenizer.json\n",
      "  vocab_size ...................................... None\n",
      "  wandb_exp_name .................................. \n",
      "  wandb_project ................................... \n",
      "  wandb_save_dir .................................. \n",
      "  weight_decay .................................... 0.1\n",
      "  weight_decay_incr_style ......................... constant\n",
      "  world_size ...................................... 1\n",
      "  yaml_cfg ........................................ None\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 32\n",
      "> building HFTokenizer tokenizer ...\n",
      " > padded vocab (size: 50277) with 27 dummy tokens (new size: 50304)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import megatron\n",
    "import sys\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_MAX_CONNECTIONS'] = '1'\n",
    "\n",
    "arg =\"pretrain_gpt2.py \\\n",
    "    --num-layers 12 \\\n",
    "    --hidden-size 768 \\\n",
    "    --num-attention-heads 12 \\\n",
    "    --seq-length 2048 \\\n",
    "    --max-position-embeddings 2048 \\\n",
    "    --micro-batch-size 4 \\\n",
    "    --global-batch-size 128 \\\n",
    "    --lr 1.5e-4 \\\n",
    "    --min-lr 1.5e-5 \\\n",
    "    --hidden-dropout 0.0 \\\n",
    "    --attention-dropout 0.0 \\\n",
    "    --train-iters 100 \\\n",
    "    --lr-decay-iters 100 \\\n",
    "    --lr-decay-style cosine \\\n",
    "    --use-parallel-residual \\\n",
    "    --weight-decay 1e-1 \\\n",
    "    --lr-warmup-fraction .01 \\\n",
    "    --clip-grad 1.0 \\\n",
    "    --normalization LayerNorm \\\n",
    "    --position-embedding-type rope \\\n",
    "    --rotary-percent 0.25 \\\n",
    "    --use-mcore-models \\\n",
    "    --untie-embeddings-and-output-weights \\\n",
    "    --tokenizer-type HFTokenizer \\\n",
    "    --vocab-file /workspace/gpt-neox-tokenizer.json \\\n",
    "    --load /data/pythia/pythia-160m-megatron \\\n",
    "    --save /data/pythia/pythia-160m-megatron/ft-test \\\n",
    "    --save-interval 100 \\\n",
    "    --finetune \\\n",
    "\"\n",
    "print(arg.split())\n",
    "sys.argv = arg.split()\n",
    "megatron.initialize_megatron(skip_mpu_initialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble to set pipelining stuff manually\n",
    "import megatron.core.parallel_state as ps\n",
    "from megatron.core.tensor_parallel.random import model_parallel_cuda_manual_seed\n",
    "ps.set_pipeline_model_parallel_rank(0)\n",
    "ps.set_pipeline_model_parallel_world_size(1)\n",
    "ps.set_tensor_model_parallel_rank(0)\n",
    "ps.set_tensor_model_parallel_world_size(1)\n",
    "ps._set_global_memory_buffer()\n",
    "model_parallel_cuda_manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 162322944\n",
      " loading checkpoint from /data/pythia/pythia-160m-megatron at iteration 1\n",
      "could not find arguments in the checkpoint ...\n",
      "Converting checkpoint from *LayerNormLinear\n",
      "Converting checkpoint from *LayerNormLinear\n",
      "loaded model with incompat keys: <All keys matched successfully>\n",
      " checkpoint version 0\n",
      " succesfully fixed query-key-values ordering for checkpoint version 0\n",
      "  successfully loaded checkpoint from /data/pythia/pythia-160m-megatron at iteration 0\n"
     ]
    }
   ],
   "source": [
    "from pretrain_gpt import train_valid_test_datasets_provider\n",
    "from megatron.training import get_model, load_checkpoint\n",
    "from megatron.core.enums import ModelType\n",
    "from pretrain_gpt import get_args, core_transformer_config_from_args, print_rank_0, GPTModel\n",
    "from megatron.core.models.gpt import gpt_layer_specs\n",
    "from megatron.global_vars import get_tokenizer\n",
    "\n",
    "args = get_args()\n",
    "def model_provider(pre_process=True, post_process=True):\n",
    "    args = get_args()\n",
    "    print_rank_0('building GPT model ...')\n",
    "    # Experimental loading arguments from yaml\n",
    "    config = core_transformer_config_from_args(args)\n",
    "    assert args.use_mcore_models\n",
    "    transformer_layer_spec = gpt_layer_specs.get_gpt_layer_local_spec(None, None)\n",
    "\n",
    "    return GPTModel(\n",
    "        config=config,\n",
    "        transformer_layer_spec=transformer_layer_spec,\n",
    "        vocab_size=args.padded_vocab_size,\n",
    "        max_sequence_length=args.max_position_embeddings,\n",
    "        pre_process=pre_process,\n",
    "        post_process=post_process,\n",
    "        fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,\n",
    "        parallel_output=True,\n",
    "        share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights,\n",
    "        position_embedding_type=args.position_embedding_type,\n",
    "        rotary_percent=args.rotary_percent,\n",
    "    )\n",
    "    \n",
    "model = get_model(model_provider, ModelType.encoder_or_decoder, wrap_with_ddp=False)\n",
    "args.iteration, args.num_floating_point_operations_so_far = load_checkpoint(model, None, None)\n",
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:megatron.core.models.common.embeddings.rotary_pos_embedding:Setting apply_rope_fusion to false because its implementation is not included in Apex. Try upgrading to the latest version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9, 50304]), torch.Size([1, 1, 9, 9]), torch.Size([1, 9]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from megatron.utils import get_ltor_masks_and_position_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens = torch.tensor(tokenizer.tokenize(\"My name is Julien and I like to\")).unsqueeze(0).cuda()\n",
    "    attention_mask, _, position_ids = get_ltor_masks_and_position_ids(\n",
    "        data=tokens,\n",
    "        eod_token=None,\n",
    "        reset_position_ids=False,\n",
    "        reset_attention_mask=False,\n",
    "        eod_mask_loss=False,\n",
    "    )\n",
    "    res = model[0](tokens, position_ids, attention_mask)\n",
    "res.shape, attention_mask.shape, position_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-160m\").float()\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-160m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[3220, 1416,  310, 9218, 1914,  285,  309,  751,  281]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      " name is Alexi and I'm to call\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hf_tokenizer_out = hf_tokenizer(\"My name is Julien and I like to\", return_tensors=\"pt\")\n",
    "    print(hf_tokenizer_out)\n",
    "    hf_res = hf_model(**hf_tokenizer_out).logits\n",
    "    print(hf_tokenizer.decode(hf_res[0].argmax(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" name is Alexi and I'm to call\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer.decode(res[0].argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input\n",
      "rotary_pos_emb\n",
      "layer.1.input_layernorm_output\n",
      "layer.1.attn_qkv_output\n",
      "layer.1.attn_post_rotary_qkv\n",
      "layer.1.post_core_attn\n",
      "layer.1.post_self_attn\n",
      "GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from megatron.core.models.gpt.gpt_model import global_buffers\n",
    "for k in list(global_buffers.keys())[:7]:\n",
    "    print(k)\n",
    "\n",
    "print(hf_model.gpt_neox.layers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcinp_res = global_buffers[\"decoder_input\"].transpose(0, 1).cpu()\n",
    "assert torch.allclose(dcinp_res, hf_model.gpt_neox.embed_in(tokens.cpu()))\n",
    "assert torch.allclose(model[0].embedding.word_embeddings.weight[tokens].cpu(), dcinp_res)\n",
    "assert torch.allclose(hf_model.gpt_neox.embed_in(tokens.cpu()), dcinp_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After input layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 768]) torch.Size([1, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hf_embin = hf_model.gpt_neox.embed_in(tokens.cpu()) \n",
    "    hf_1inp_ln = hf_model.gpt_neox.layers[0].input_layernorm(hf_embin)\n",
    "mg_1inp_ln = global_buffers[\"layer.1.input_layernorm_output\"].cpu().transpose(0,1).contiguous()\n",
    "print(hf_1inp_ln.shape, mg_1inp_ln.shape)\n",
    "\n",
    "assert torch.allclose(hf_1inp_ln, mg_1inp_ln, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1574, -0.4843,  0.2528,  0.5275,  0.1467, -0.0559, -0.1719, -0.1283,\n",
      "        -0.1033, -0.1658])\n",
      "tensor([-0.1574, -0.4843,  0.2528,  0.5275,  0.1467, -0.0559, -0.1719, -0.1283,\n",
      "        -0.1033, -0.1658])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    hf_1attn_out,_ = hf_model.gpt_neox.layers[0].attention(hf_1inp_ln, attention_mask=hf_tokenizer_out[\"attention_mask\"], position_ids=None)\n",
    "    hf_1attn_out += hf_embin\n",
    "\n",
    "mg_1attn_out = global_buffers[\"layer.1.post_self_attn\"].cpu().transpose(0,1)\n",
    "print(hf_1attn_out[0,0,:10])\n",
    "print(mg_1attn_out[0,0,:10])\n",
    "assert torch.allclose(mg_1attn_out, hf_1attn_out, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After attention QKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 2304]) torch.Size([1, 9, 2304])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hf_1qkv = hf_model.gpt_neox.layers[0].attention.query_key_value(hf_1inp_ln)\n",
    "mg_1qkv = global_buffers[\"layer.1.attn_qkv_output\"].transpose(0,1).cpu()\n",
    "print(hf_1qkv.shape, mg_1qkv.shape)\n",
    "assert torch.allclose(hf_1qkv, mg_1qkv, atol=1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After rotary embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "# Copied from transformers.models.mistral.modeling_mistral.apply_rotary_pos_emb\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`):\n",
    "            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n",
    "            used to pass offsetted position ids when working with a KV-cache.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "def qkv_forward(self, qkv, layer_past=None):\n",
    "    position_ids = torch.arange(qkv.shape[1], dtype=torch.long, device=qkv.device)\n",
    "    position_ids = position_ids.unsqueeze(0)\n",
    "    has_layer_past = layer_past is not None\n",
    "    new_qkv_shape = qkv.size()[:-1] + (self.num_attention_heads, 3 * self.head_size)\n",
    "    qkv = qkv.view(*new_qkv_shape)\n",
    "\n",
    "    # [batch, seq_len, num_attention_heads, 3 * head_size] --> 3 [batch, num_attention_heads, seq_len, head_size]\n",
    "    query = qkv[..., : self.head_size].permute(0, 2, 1, 3)\n",
    "    key = qkv[..., self.head_size : 2 * self.head_size].permute(0, 2, 1, 3)\n",
    "    value = qkv[..., 2 * self.head_size :].permute(0, 2, 1, 3)\n",
    "\n",
    "    # Compute rotary embeddings on rotary_ndims\n",
    "    query_rot = query[..., : self.rotary_ndims]\n",
    "    query_pass = query[..., self.rotary_ndims :]\n",
    "    key_rot = key[..., : self.rotary_ndims]\n",
    "    key_pass = key[..., self.rotary_ndims :]\n",
    "\n",
    "    # Compute token offset for rotary embeddings (when decoding)\n",
    "    seq_len = key.shape[-2]\n",
    "    if has_layer_past:\n",
    "        seq_len += layer_past[0].shape[-2]\n",
    "    cos, sin = self.rotary_emb(value, seq_len=seq_len)\n",
    "    query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n",
    "    query = torch.cat((query, query_pass), dim=-1)\n",
    "    key = torch.cat((key, key_pass), dim=-1)\n",
    "    return query, key, value\n",
    "\n",
    "hf_1 = hf_model.gpt_neox.layers[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    hf_1post_rotary = qkv_forward(hf_1.attention, hf_1qkv)\n",
    "    mg_1post_rotary = list(global_buffers[\"layer.1.attn_post_rotary_qkv\"])\n",
    "    for i in range(3):\n",
    "        mg_1post_rotary[i] = mg_1post_rotary[i].permute(1, 2, 0, 3).contiguous().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 9, 64]) torch.Size([1, 12, 9, 64])\n",
      "torch.Size([1, 12, 9, 64]) torch.Size([1, 12, 9, 64])\n",
      "torch.Size([1, 12, 9, 64]) torch.Size([1, 12, 9, 64])\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(hf_1post_rotary[i].shape, mg_1post_rotary[i].shape)\n",
    "    assert torch.allclose(hf_1post_rotary[i], mg_1post_rotary[i], atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Core Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 768]) torch.Size([1, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hf_1attn = hf_1.attention\n",
    "    hf_1attn_output, hf_1attn_weights = hf_1attn._attn(*hf_1post_rotary, hf_tokenizer_out[\"attention_mask\"], None)\n",
    "    hf_1attn_output = hf_1attn._merge_heads(hf_1attn_output, hf_1attn.num_attention_heads, hf_1attn.head_size)\n",
    "\n",
    "mg_1attn_postcore = global_buffers[\"layer.1.post_core_attn\"].transpose(0,1).cpu()\n",
    "print(hf_1attn_output.shape, mg_1attn_postcore.shape)\n",
    "assert torch.allclose(hf_1attn_output[0,0], mg_1attn_postcore[0,0], atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post linear projection bias/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hf_1dense_output = hf_1attn.dense(hf_1attn_output)\n",
    "    mg_1dense_output = global_buffers[\"layer.1.attn_dense_output\"].transpose(0,1).cpu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
